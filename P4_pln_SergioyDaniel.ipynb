{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUBRwgEu3yu1"
   },
   "source": [
    "# Práctica 4: Procesamiento del Lenguaje Natural\n",
    "\n",
    "__Fecha de entrega: 16 de mayo de 2025__\n",
    "\n",
    "El objetivo de esta práctica es aplicar los conceptos teóricos vistos en clase en el módulo de PLN.\n",
    "\n",
    "Lo más importante en esta práctica no es el código Python, sino el análisis de los datos y modelos que construyas y las explicaciones razonadas de cada una de las decisiones que tomes. __No se valorarán trozos de código o gráficas sin ningún tipo de contexto o explicación__.\n",
    "\n",
    "Finalmente, recuerda establecer el parámetro `random_state` en todas las funciones que tomen decisiones aleatorias para que los resultados sean reproducibles (los resultados no varíen entre ejecuciones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRUPO K: Sergio Martínez Olivera y Daniel Roldan Serrano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "V3YxCTUW3yu9"
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeVD_g2D3yvC"
   },
   "source": [
    "# 1) Carga del conjunto de datos\n",
    "\n",
    "Los ficheros `fake.csv` y `true.csv` contienen artícuos de noticias clasificadas como fake (falsas) o true (reales) respectivamente. Cada noticia tiene como atributos:\n",
    "\n",
    "*   Title: título de la noticia\n",
    "*   Text: cuerpo del texto de la noticia\n",
    "*   Subject: tema de la noticia\n",
    "*   Date: fecha de publicación de la noticia\n",
    "\n",
    "Muestra un ejemplo de cada clase.\n",
    "\n",
    "Haz un estudio del conjunto de datos. ¿qué palabras aparecen más veces?, ¿tendría sentido normalizar de alguna manera el corpus?\n",
    "\n",
    "Crea una partición de los datos dejando el 60% para entrenamiento, 20% para validación y el 20% restante para test. Comprueba que la distribución de los ejemplos en las particiones es similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\danie\\tf-env\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\danie\\tf-env\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\danie\\tf-env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\danie\\tf-env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\danie\\tf-env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\danie\\tf-env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\danie\\tf-env\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\danie\\tf-env\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\danie\\tf-env\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\danie\\tf-env\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\danie\\tf-env\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\danie\\tf-env\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\danie\\tf-env\\lib\\site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\danie\\tf-env\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\danie\\tf-env\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\danie\\tf-env\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\danie\\tf-env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\danie\\tf-env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Realizamos todos los imports necesarios\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leemos los csv y añadimos una columna a cada data frame que indica el tipo de noticia\n",
    "fake_df  = pd.read_csv('fake.csv')\n",
    "true_df  = pd.read_csv('true.csv')\n",
    "fake_df['type'] = 0  # Fake news\n",
    "true_df['type'] = 1  # True news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  type  \n",
       "0  December 31, 2017     0  \n",
       "1  December 31, 2017     0  \n",
       "2  December 30, 2017     0  \n",
       "3  December 29, 2017     0  \n",
       "4  December 25, 2017     0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df.head() #Mostramos los primeros elemenos de fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  type  \n",
       "0  December 31, 2017      1  \n",
       "1  December 29, 2017      1  \n",
       "2  December 31, 2017      1  \n",
       "3  December 30, 2017      1  \n",
       "4  December 29, 2017      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_df.head() #Mostramos los primeros elementos de true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "#Juntamos ambos dataframes en uno solo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = np.array(df[\"text\"])\n",
    "tipo = np.array(df[\"type\"])\n",
    "#Convertimos a array las columnas \"text\" y \"type\" del dataframe conjunto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiene sentido normalizar, ya que, en caso contrario, estaríamos teniendo en cuenta caracteres especiales, espacios en blanco y las conocidas como stopwords (preposiciones, arículos...), que no tienen ningún poder discriminante en nuestro objetivo, que es interpretar y clasificar el texto en función de sus palabras. Lo realizamos mediante la función normalize_document, que, aparte de lo comentado, pone todas las palabras en minúsculas, para evitar distinguir palabras con iguales pero con primera letra mayúscula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_news = normalize_corpus(news) #Normalizamos el documento news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('said', 130100), ('trump', 116723), ('us', 62643), ('would', 54958), ('president', 51202), ('people', 41107), ('one', 35661), ('state', 31362), ('also', 31202), ('new', 30976)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "all_text = ' '.join(norm_news) #Unimos todos los textos\n",
    "\n",
    "words = all_text.split() #Separamos el texto conjunto en todas sus palabras\n",
    "\n",
    "conteo = Counter(words) #Contamos las apariciones de cada palabra \n",
    "\n",
    "print(conteo.most_common(10))  #Mostramos las 10 palabras que más aparecen junto con su número de apariciones. Entre ellas, algunas\n",
    "#son esperables (said, us, would, also...) que son adverbios, verbos auxiliares, etc. Las más destacables fuera de estas son trump, president,\n",
    "#state y people, lo cual tiene sentido ya que la temática de todos los textos gira en torno a Estados Unidos y su política."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_resto, y_train, y_resto = train_test_split(news, tipo, test_size=0.4, random_state=RANDOM_STATE, stratify = tipo)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_resto, y_resto, test_size=0.5, random_state=RANDOM_STATE, stratify = y_resto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para llevar a cabo la división de los datos en la partición que exige el enunciado, primero, realizamos una primera división con el 60% para determinar los datos del conjunto de entrenamiento (X_train, y_train) y después, sobre el conjunto restante (X_resto, y_resto) realizamos una segunda división de 50% para dividir el resto en test y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training texts: 26938\n",
      "Test texts: 8980\n",
      "Validation texts: 8980\n",
      "Proporción de fake en train:  0.5229786918108249\n",
      "Proporción de fake en val:  0.5230512249443207\n",
      "Proporción de fake en test:  0.5229398663697105\n"
     ]
    }
   ],
   "source": [
    "print(\"Training texts:\", len(y_train))\n",
    "print(\"Test texts:\", len(y_test))\n",
    "print(\"Validation texts:\", len(y_val))\n",
    "\n",
    "print(\"Proporción de fake en train: \" , np.count_nonzero(y_train == 0)/np.size(y_train))\n",
    "print(\"Proporción de fake en val: \" , np.count_nonzero(y_val == 0)/np.size(y_val))\n",
    "print(\"Proporción de fake en test: \" , np.count_nonzero(y_test == 0)/np.size(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con unos simples cálculos comprobamos que la proporción de 60%, 20% y 20% para entrenamiento, validación y test respectivamente, está bien hecha, y observamos que el porcentaje de fake news en cada conjunto de la partición es prácticamente el mismo porque incluimos stratify = tipo y \n",
    "stratify = y_resto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4rXv3xX3yvG"
   },
   "source": [
    "# 2) Representación como bolsa de palabras\n",
    "\n",
    "Elige justificadamente una representación de bolsa de palabras y aplícala.\n",
    "Muestra un ejemplo antes y después de aplicar la representación. Explica los cambios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = normalize_corpus(X_train)\n",
    "X_val_norm = normalize_corpus(X_val)\n",
    "X_test_norm = normalize_corpus(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['release nude pictures might meant distraction trump disastrous two weeks beginning democratic national convention revealed melania trump may modeling without proper documentation words gasp may illegal immigrant appears strange things happening melania immigration status might married another american four years becoming mrs trumpthe story broken univision proving taking away reporter access backfire candidate claim immigration attorney worked trump organization said melania obtained green card based marriage four years married trumpbut michael wildes immigration attorney worked trump organization told univision investigative unit obtained green card four years earlier based marriage melania donald trump married jan bethesdabythe sea episcopal church palm beach floridawildes asked trump campaign far cricketswhen asked explain marriage discrepancy wildes said would seek clarification presumably trump organization later sent email saying hear back sorry wildes someone know immigration attorney trump models miss universe pageant run trump according univision defector trump fact authorized trump campaign speak behalfstill wildes says things discuss privacy issuesthe clinton campaign officially anyway refusing make anything story one former federal prosecutor hillary supporter said moot ridiculous videothe clinton supporter essentially right point complete hypocrisy trump campaign trump care immigration issues never trump almost prefers hire immigrants americans well marriage history proves prefers wives come elsewhere sure democrats would happy drop issue exchange received little intellectual honesty trump campaign instead trump continue bashing immigrants even people born america immigrant parents going home night woman well might one illegal immigrants trump supporters love rant',\n",
       "       'washington reuters white house confirmed monday ben rhodes deputy national security adviser would testify house oversight panel iran tuesday panel asked rhodes appear new york times article suggested manipulated public debate iran deal',\n",
       "       'st century wire says classic case problem reaction solution far federal government go dismantle us constitution back establishment rigged us presidential election made florida notorious pregnant chads centerpiece artificial controversy predictably public reaction outrage suddenly calls nationwide electronic voting solution old inefficient paper ballot systemnow electronic voting allpervasive us next problem becomes cyber security government referring next burning national security issueenter relatively new prohibitively expensive department homeland security dhs ironically created one george w bush president installed power popular vote dubious supreme court decision us presidential election dhs head obama appointee jeh johnson nudging towards idea domestic military force taking election process johnson claims us elections part country vital infrastructure therefore must control already bloated federal agencywhat wrong decentralized traditional paper ballot system business fixing elections answer simple paper difficult sloppy complicated hackit coincidence latest powergrab federal government comes immediately mainstream conspiracy theory russia somehow behind week election database hack arizona putin proof offered either politicians media anchors presently parroting latest sensational plotlooking back see years excessive lobbying washington insider firms eager tosecure lucrative contracts implementing myriad fatally flawed electronic voting systems mere accident nature designed police state endrun see nowthe problem would unconstitutional government really care constitutional lawnot erecting police state barbara hollingsworth cns newshomeland security secretary jeh johnson suggestion state local voting systems designated pieces critical infrastructure us department homeland security dhs protect hackers unconstitutional would create dangerous precedent legal experts said department homeland security legal authority interfere states election systems without permission university californiaberkeley school lawprofessor john yoo told cnsnewscom federal government general power protect nation cyber infrastructure cannot intrude areas state sovereignty without clear constitutional mandate article section constitution recognizes authority states regulate times places manner elections subject congressional regulation far aware congress clearly decided regulate information systems state electoral systems delegated authority dhs yoo noted continue story cns newsread police state news st century wire police state files',\n",
       "       ...,\n",
       "       'texas conservatives cheering britain voted leave european union believe means secede united statesupon learning results conservatives took social media urge texas declare independencetexas leave failed american union rejoin world independent selfgoverning nation texit httpstcoesvroei trey mays treymays june texas louisiana oklahoma secede form new country called united states awesome texit collin scopy crapplefratz june event hillaryclinton becomes potus govabbott begin plans texit thanks superb leadership tcot stan weber spweber june please please please vote texas leave us texit jeff tiedrich jefftiedrich june join fellow texans pledge vote texas independencehttpstcouuashvoxtexit pictwittercommcllf tnm texasnatmov june forget brexit want know going start texit pictwittercomrabkrgwxy modern rebel mamericanrebel june one happier british exit eu texas nationalist movement president daniel millerin interview guardian miller claimed britain leaving eu exact texas wanting leave united states could take britain replace texas could take eu replace us could take brussels replace washington dc could give guys nice texas drawl one would know different much exactly vast majority laws rules regulations affect people texas created political class unelected bureaucrats washington continuedit likely texas able secede however shortly civil war supreme court ruled texas v whitewhen therefore texas became one united states entered indissoluble relation obligations perpetual union guaranties republican government union attached state act consummated admission union something compact incorporation new member political body final union texas states complete perpetual indissoluble union original states place reconsideration revocation except revolution consent statesin short way texas secede majority states agree let texas soand perhaps time let themthe republican party would suffer serious blow texas exited union texas electoral votes sends representatives congress republicans two senators texas sends congress also republicans would democrats win presidency handily could take back house senatefurthermore united states government could stop sending federal dollars texasyou also bet conservatives would flock texas search conservative paradiseand new country texas becomes failed state build wall around let deal mess instead coming rescue america texas begged entrance united states severe debtit time cut cord america texas dragging country long survived nation nearly years without texas need texas definitely need texas republican party needs texas let kill two birds one stone let texas go texas goes goes republican party texas entered country call marriage convenience would divorce convenience except america would finally progress future deserve texas continues go backwardsfeatured image via wikimedia',\n",
       "       'whole world knows investigation donald trump campaign ties russia likely led firing fbi director james comey however new reporting also reveals comey really think much trump first place according report week comey expressed concern trump temperament related presidency leadership nation allegedly told confidantes trump crazy outside realm normal like rest sane people america comey also seemed believe trump lack self restraint impulsive reckless statements twitter danger nation seems trump angry comey several occasions well comey testified open congressional hearing evidence corroborate trump claim president obama wiretapped trump tower trump incensed true comey refusal pledge loyalty trump instead united states constitutiondaniel c richman close adviser comey says situation president seems prize personal loyalty else director absolute commitment constitution pursuing investigations wherever evidence led collision bound happen indeed problem trump give damn constitution rule law fact seems care benefits makes look good care american people nation whole sorry excuse presidency nothing ultimate reality show incompetent unfit fool biggest ego world unfortunately paying pricefeatured image via andrew harrerpoolgetty images',\n",
       "       'washington post reporter took twitter post image venue trump appear several hours problem washington post reporter say image taken several hours president trump appeared stagehere dave weigel fakenews post since deletedas someone attended trump final campaign rally night election grand rapids mi attest incredible amount security trump supporters must go entering venue trump supporters lined miles get event attended several hours many supporters seen trickling hours event started waiting get securitydaveweigel washingtonpost put phony photo empty arena hours arrived venue w thousands people outside way real photos shown spoke packed house many people unable get demand apology retraction fake news wapo pictwittercomxablfghob donald j trump realdonaldtrump december pictures trump posted tweet show actual size crowd washington post reporter outed president trump lying twitter size trump crowd rally apologizedsure thing apologize deleted photo dmartosko told id gotten wrong confused image walking bottom right corner httpstcofqygmnsad dave weigel daveweigel december president trump responded dave weigel apology twitter trump called firing pushing fakenews twitterdaveweigel washington post admitted picture fake fraud showing almost empty arena last night speech pensacola fact knew arena packed shown also tv fake news fired donald j trump realdonaldtrump december many trump supporters came defense twitter well posting images packed stadium everyone seehere line outside trump rally actually looked likehappening nowtrump rally pensacolapictwittercomoqupd trump news michaeldelauzon december several shots pensacola venue insidestadium packed trump rally pensacola flwe arent going anywhere getting started make america great thingsitrustmorethancnn resistanceisimportant trumprally pictwittercomrcsugdo kambree kawahine koa kamvtv december saying pensacola bay center empty tonight trump rally pictwittercomzzqvykxif bard law twolameducks december crowds gather trump rally pensacola today pictwittercomerjynzm ssd pismob december video view inside pensacola trump rally tonight president talked illegal immigration pictwittercomvopbebqije david martosko dmartosko december'],\n",
       "      dtype='<U40960')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_norm #Representación de los textos del conjunto de entrenamiento como un array en el que cada posición es un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#CountVectorizer es una opción de bolsa de palabras, que muestra la frecuencia de aparición de cada palabra en cada texto.\n",
    "\n",
    "#Estas matrices de CountVectorizer se declaran por si se requiere de su uso en el futuro (sin límite de max_features), \n",
    "#ya que para su visualización debemos imponer un max_features al Count Vectorizer para que no se supere la memoria permitida del ordenador.\n",
    "cv = CountVectorizer()\n",
    "cv_train_matrix = cv.fit_transform(X_train_norm)\n",
    "\n",
    "cv_test_matrix = cv.transform(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>...</th>\n",
       "      <th>zika</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zimmerman</th>\n",
       "      <th>zinke</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26933</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26934</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26935</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26936</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26937</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26938 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       000  10  11  12  13  14  15  17  18  19  ...  zika  zimbabwe  \\\n",
       "0        0   0   0   0   0   0   0   0   0   0  ...     0         0   \n",
       "1        0   0   0   0   0   0   0   0   0   0  ...     0         0   \n",
       "2        0   0   0   0   0   0   0   0   0   0  ...     0         0   \n",
       "3        0   0   0   0   0   0   0   0   0   0  ...     0         0   \n",
       "4        0   0   0   0   0   0   0   0   0   0  ...     0         0   \n",
       "...    ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...   ...       ...   \n",
       "26933    0   0   0   0   0   0   0   0   0   0  ...     0         0   \n",
       "26934    0   0   0   0   0   0   0   0   0   0  ...     0         0   \n",
       "26935    0   0   0   0   0   0   0   0   0   0  ...     0         0   \n",
       "26936    0   0   0   0   0   0   0   0   0   0  ...     0         0   \n",
       "26937    0   0   0   0   0   0   0   0   0   0  ...     0         0   \n",
       "\n",
       "       zimbabwean  zimmerman  zinke  zip  zone  zones  zuckerberg  zuma  \n",
       "0               0          0      0    0     0      0           0     0  \n",
       "1               0          0      0    0     0      0           0     0  \n",
       "2               0          0      0    0     0      0           0     0  \n",
       "3               0          0      0    0     0      0           0     0  \n",
       "4               0          0      0    0     0      0           0     0  \n",
       "...           ...        ...    ...  ...   ...    ...         ...   ...  \n",
       "26933           0          0      0    0     0      0           0     0  \n",
       "26934           0          0      0    0     0      0           0     0  \n",
       "26935           0          0      0    0     0      0           0     0  \n",
       "26936           0          0      0    0     0      0           0     0  \n",
       "26937           0          0      0    0     0      0           0     0  \n",
       "\n",
       "[26938 rows x 10000 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#La delcaración de estas matrices de CountVectorizer sirven para visualizar la matriz, y para ello incluimos un max_features\n",
    "#que permite no sobrepasar la memoria máxima permitida del ordenador.\n",
    "#Para ello, Determinamos el vocabulario (conjunto de todas las palabras distintas de los textos) y convertimos la matriz del \n",
    "#CountVectorizer a  array para poder visualizarla.\n",
    "cv2 = CountVectorizer(max_features = 10000)\n",
    "cv_train_matrix_visualize = cv2.fit_transform(X_train_norm)\n",
    "cv_train_matrix_visualize = cv_train_matrix_visualize.toarray()\n",
    "vocab = cv2.get_feature_names_out()\n",
    "pd.DataFrame(cv_train_matrix_visualize, columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>said</th>\n",
       "      <th>trump</th>\n",
       "      <th>us</th>\n",
       "      <th>would</th>\n",
       "      <th>president</th>\n",
       "      <th>people</th>\n",
       "      <th>one</th>\n",
       "      <th>state</th>\n",
       "      <th>also</th>\n",
       "      <th>new</th>\n",
       "      <th>reuters</th>\n",
       "      <th>donald</th>\n",
       "      <th>states</th>\n",
       "      <th>house</th>\n",
       "      <th>government</th>\n",
       "      <th>clinton</th>\n",
       "      <th>obama</th>\n",
       "      <th>republican</th>\n",
       "      <th>could</th>\n",
       "      <th>told</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   said  trump  us  would  president  people  one  state  also  new  reuters  \\\n",
       "0     3     17   0      2          0       1    2      0     0    0        0   \n",
       "1     0      0   0      1          0       0    0      0     0    1        1   \n",
       "2     1      0   6      2          1       0    1      7     0    1        0   \n",
       "3     0      0   0      0          0       0    0      0     0    0        0   \n",
       "4     1      0   0      0          0       1    0      0     0    1        0   \n",
       "\n",
       "   donald  states  house  government  clinton  obama  republican  could  told  \n",
       "0       1       0      0           0        2      0           0      0     1  \n",
       "1       0       0      2           0        0      0           0      0     0  \n",
       "2       0       2      0           5        0      1           0      0     1  \n",
       "3       0       0      0           0        0      0           0      0     0  \n",
       "4       0       0      0           0        0      0           0      0     0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Para visualizar mejor el funcionamiento de Count Vectorizer, mostramos la frecuencia de aparición de las 20 palabras que aparecen \n",
    "#con más frecuencia en los primeros textos, porque de lo contrario, observaremos mayoritariamente ceros en todas las casillas de la matriz.\n",
    "\n",
    "# Sumamos las frecuencias de cada palabra en todos los documentos\n",
    "frecuencia_global = np.sum(cv_train_matrix_visualize, axis=0)\n",
    "\n",
    "# Elegimos las 20 palabras más frecuentes\n",
    "N = 20\n",
    "indices_top = np.argsort(frecuencia_global)[::-1][:N]\n",
    "\n",
    "# Creamos un DataFrame con las 20 palabras más comunes\n",
    "df_top = pd.DataFrame(cv_train_matrix_visualize[:, indices_top], columns=vocab[indices_top])\n",
    "df_top.head()  # Mostramos los primeros textos con la frecuencia de aparición de las 20 palabras más comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>...</th>\n",
       "      <th>zika</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zimbabwean</th>\n",
       "      <th>zimmerman</th>\n",
       "      <th>zinke</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26933</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26934</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26935</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26936</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26937</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26938 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       000   10   11   12   13   14   15   17   18   19  ...  zika  zimbabwe  \\\n",
       "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0       0.0   \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0       0.0   \n",
       "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0       0.0   \n",
       "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0       0.0   \n",
       "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0       0.0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...       ...   \n",
       "26933  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0       0.0   \n",
       "26934  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0       0.0   \n",
       "26935  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0       0.0   \n",
       "26936  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0       0.0   \n",
       "26937  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0       0.0   \n",
       "\n",
       "       zimbabwean  zimmerman  zinke  zip  zone  zones  zuckerberg  zuma  \n",
       "0             0.0        0.0    0.0  0.0   0.0    0.0         0.0   0.0  \n",
       "1             0.0        0.0    0.0  0.0   0.0    0.0         0.0   0.0  \n",
       "2             0.0        0.0    0.0  0.0   0.0    0.0         0.0   0.0  \n",
       "3             0.0        0.0    0.0  0.0   0.0    0.0         0.0   0.0  \n",
       "4             0.0        0.0    0.0  0.0   0.0    0.0         0.0   0.0  \n",
       "...           ...        ...    ...  ...   ...    ...         ...   ...  \n",
       "26933         0.0        0.0    0.0  0.0   0.0    0.0         0.0   0.0  \n",
       "26934         0.0        0.0    0.0  0.0   0.0    0.0         0.0   0.0  \n",
       "26935         0.0        0.0    0.0  0.0   0.0    0.0         0.0   0.0  \n",
       "26936         0.0        0.0    0.0  0.0   0.0    0.0         0.0   0.0  \n",
       "26937         0.0        0.0    0.0  0.0   0.0    0.0         0.0   0.0  \n",
       "\n",
       "[26938 rows x 10000 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#TF-IDF es otra opción de bolsa de palabras, que muestra la relación TF-IDF de cada palabra en cada texto (posteriormente\n",
    "#hablaremos más sobre ella).\n",
    "\n",
    "#Estas primeras matrices se declaran por si se requieren de su uso posteriormente en la práctica (no tienen límite de max_features).\n",
    "\n",
    "tv = TfidfVectorizer()\n",
    "tv_train_matrix = tv.fit_transform(X_train_norm)\n",
    "tv_test_matrix = tv.transform(X_test_norm)\n",
    "\n",
    "#Para visualizar cómo funciona TF-IDF, declaramos matrices usando TF-IDF con max_features, para no sobrepasar la memoria máxima permitida\n",
    "#del ordenador, al igual que en CountVectorizer.\n",
    "tv2 = TfidfVectorizer(max_features = 10000)\n",
    "tv_matrix_visualize = tv2.fit_transform(X_train_norm)\n",
    "vocab = tv2.get_feature_names_out()\n",
    "tv_matrix_visualize = tv_matrix_visualize.toarray()\n",
    "pd.DataFrame(np.round(tv_matrix_visualize, 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trump</th>\n",
       "      <th>said</th>\n",
       "      <th>us</th>\n",
       "      <th>president</th>\n",
       "      <th>would</th>\n",
       "      <th>people</th>\n",
       "      <th>clinton</th>\n",
       "      <th>house</th>\n",
       "      <th>state</th>\n",
       "      <th>obama</th>\n",
       "      <th>reuters</th>\n",
       "      <th>one</th>\n",
       "      <th>donald</th>\n",
       "      <th>republican</th>\n",
       "      <th>new</th>\n",
       "      <th>white</th>\n",
       "      <th>states</th>\n",
       "      <th>government</th>\n",
       "      <th>united</th>\n",
       "      <th>also</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trump  said    us  president  would  people  clinton  house  state  obama  \\\n",
       "0   0.34  0.05  0.00       0.00   0.04    0.02     0.07   0.00   0.00   0.00   \n",
       "1   0.00  0.00  0.00       0.00   0.06    0.00     0.00   0.17   0.00   0.00   \n",
       "2   0.00  0.01  0.11       0.02   0.03    0.00     0.00   0.00   0.16   0.03   \n",
       "3   0.00  0.00  0.00       0.00   0.00    0.00     0.00   0.00   0.00   0.00   \n",
       "4   0.00  0.02  0.00       0.00   0.00    0.02     0.00   0.00   0.00   0.00   \n",
       "5   0.00  0.00  0.00       0.00   0.02    0.03     0.00   0.00   0.03   0.00   \n",
       "6   0.00  0.03  0.00       0.00   0.03    0.02     0.06   0.00   0.00   0.00   \n",
       "7   0.00  0.02  0.00       0.00   0.00    0.00     0.00   0.00   0.00   0.00   \n",
       "8   0.00  0.03  0.00       0.00   0.00    0.00     0.00   0.00   0.00   0.00   \n",
       "9   0.00  0.02  0.01       0.01   0.00    0.06     0.00   0.00   0.07   0.00   \n",
       "\n",
       "   reuters   one  donald  republican   new  white  states  government  united  \\\n",
       "0     0.00  0.04    0.02        0.00  0.00   0.00    0.00        0.00    0.00   \n",
       "1     0.06  0.00    0.00        0.00  0.08   0.09    0.00        0.00    0.00   \n",
       "2     0.00  0.02    0.00        0.00  0.02   0.00    0.05        0.12    0.00   \n",
       "3     0.00  0.00    0.00        0.00  0.00   0.00    0.00        0.00    0.00   \n",
       "4     0.00  0.00    0.00        0.00  0.02   0.00    0.00        0.00    0.00   \n",
       "5     0.00  0.00    0.00        0.00  0.03   0.00    0.00        0.10    0.00   \n",
       "6     0.00  0.00    0.00        0.00  0.00   0.00    0.02        0.00    0.02   \n",
       "7     0.00  0.02    0.00        0.00  0.00   0.00    0.00        0.00    0.00   \n",
       "8     0.00  0.00    0.00        0.00  0.00   0.00    0.00        0.00    0.00   \n",
       "9     0.00  0.01    0.00        0.02  0.03   0.00    0.00        0.00    0.00   \n",
       "\n",
       "   also  \n",
       "0  0.00  \n",
       "1  0.00  \n",
       "2  0.00  \n",
       "3  0.00  \n",
       "4  0.00  \n",
       "5  0.00  \n",
       "6  0.00  \n",
       "7  0.06  \n",
       "8  0.00  \n",
       "9  0.03  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Para visualizar mejor el funcionamiento de TF-IDF, mostramos la frceuencia de aparición de las 20 palabras que aparecen con más frecuencia\n",
    "#en los primeros textos, porque de lo contrario, observaremos mayoritariamente ceros en todas las casillas de la matriz.\n",
    "\n",
    "# Calculamos TF-IDF promedio para cada palabra en todo el corpus\n",
    "tfidf_promedio = np.mean(tv_matrix_visualize, axis=0)\n",
    "\n",
    "# Obtenemos las 20 palabras con mayor promedio\n",
    "N = 20\n",
    "top_indices = np.argsort(tfidf_promedio)[::-1][:N]\n",
    "\n",
    "# Mostramos solo esas palabras para los primeros textos\n",
    "df_tfidf_top = pd.DataFrame(\n",
    "    np.round(tv_matrix_visualize[:10, top_indices], 2),\n",
    "    columns=vocab[top_indices]\n",
    ")\n",
    "df_tfidf_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre el CountVectorizer y el TF-IDF, vamos a optar por la representación en una matriz del valor TF-IDF de cada palabra en cada texto,\n",
    "ya que se trata de un valor mucho más concluyente y más representativo que la frecuencia en sí, ya que resta importancia a las palabras que\n",
    "aparecen con bastante frecuencia en todos los textos (adverbios, verbos auxiliares,...) y otroga más importancia a las que aparecen con frecuencia pero solo en uno de los textos, incluso tras normalizar. Por ejemplo, en el texto 2 would aparece 2 veces, mientras que Obama aparece 1, pero sin embargo, ambos tienen el mismo valor de TF-IDF, 0.03. Un caso similar ocurre con government, que aparece 5 veces, pero sin embargo, cuatriplica su valor de TF-IDF respecto de would, 0.12 frente a 0.03, y sin embargo, no aparece 4 veces más que would. Esto se debe a que would, al ser un verbo auxiliar, aparece con frecuencia en todos los textos y TF-IDF le resta importancia.\n",
    "Por otro lado, es evidente que la representación con cualquiera de las dos bolsas de palabras es mejor que la representación como array de textos, ya que, visualizando ambos casos, como pide el enunciado, en la bolsa de palabras tenemos una imagen mucho más ordenada y entendible de las propiedades de cada texto en función de sus palabras, y muy fácil de estudiar observando filas y coolumnas, mientras que si simplemente almacenamos los textos en un array (X_train_norm) no podemos concluir ni deducir nada acerca de ellos con tan solo observarlos, requeriría de un estudio y lectura detallada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHAawx3OHoiI"
   },
   "source": [
    "# 3) Aplica 3 algoritmos de aprendizaje automático para resolver la tarea\n",
    "\n",
    "Justifica porqué los has elegido.\n",
    "Ajusta los modelos respecto a un hiperparámetro que consideres oportuno. Justifica tu elección.\n",
    "Explica los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya hemos mencionado, para los 3 algoritmos utilizaremos TF-IDF como bolsa de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Árbol, porcentaje de aciertos en entrenamiento con max_depth =  3  : 0.9944316578810602\n",
      "Árbol, porcentaje de aciertos en test con max_depth =  3  : 0.9920935412026726\n",
      "Árbol, porcentaje de aciertos en entrenamiento con max_depth =  5  : 0.996176405078328\n",
      "Árbol, porcentaje de aciertos en test con max_depth =  5  : 0.9938752783964365\n",
      "Árbol, porcentaje de aciertos en entrenamiento con max_depth =  10  : 0.9980696413987675\n",
      "Árbol, porcentaje de aciertos en test con max_depth =  10  : 0.9937639198218263\n",
      "Árbol, porcentaje de aciertos en entrenamiento con max_depth =  15  : 0.9987378424530403\n",
      "Árbol, porcentaje de aciertos en test con max_depth =  15  : 0.9933184855233853\n",
      "Árbol, porcentaje de aciertos en entrenamiento con max_depth =  20  : 0.9992204321033484\n",
      "Árbol, porcentaje de aciertos en test con max_depth =  20  : 0.9935412026726058\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "import numpy as np\n",
    "\n",
    "depth_values = [3,5,10,15,20];\n",
    "#En este caso, hemos elegido max_depth como hiperparámetro a ajustar ya que determina la profundidad maxima del arbol de decisión, y \n",
    "#por lo tanto, tiene un gran efecto sobre el overfitting/underfitting del algoritmo. Otra opción era min_samples_split y min_samples_leaf,\n",
    "#que controlan el número de muestras en las hojas del árbol.\n",
    "\n",
    "for i in range (0,5):\n",
    "    tree_classifier = tree.DecisionTreeClassifier(max_depth = depth_values[i])\n",
    "    tree_classifier.fit(tv_train_matrix, y_train)\n",
    "\n",
    "    tree_train_predictions = tree_classifier.predict(tv_train_matrix)\n",
    "    tree_test_predictions = tree_classifier.predict(tv_test_matrix)\n",
    "\n",
    "    print(\"Árbol, porcentaje de aciertos en entrenamiento con max_depth = \",depth_values[i], \" :\", np.mean(tree_train_predictions == y_train))\n",
    "    print(\"Árbol, porcentaje de aciertos en test con max_depth = \",depth_values[i], \" :\",np.mean(tree_test_predictions == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, en todos los casos se observa un gran rendimiento del algoritmo, independientemente del valor de max_depth utilizado.\n",
    "De hecho, fijándonos en el tercer decimal, hay una tendencia a mayor rendimiento cuanto mayor sea el valor de max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN, porcentaje de aciertos en entrenamiento con k =  2  : 0.7230677852847279\n",
      "k-NN, porcentaje de aciertos en test con k =  2  : 0.6478841870824054\n",
      "k-NN, porcentaje de aciertos en entrenamiento con k =  4  : 0.6437003489494395\n",
      "k-NN, porcentaje de aciertos en test con k =  4  : 0.6140311804008909\n",
      "k-NN, porcentaje de aciertos en entrenamiento con k =  6  : 0.6114782092211746\n",
      "k-NN, porcentaje de aciertos en test con k =  6  : 0.5938752783964365\n",
      "k-NN, porcentaje de aciertos en entrenamiento con k =  8  : 0.5923973568936075\n",
      "k-NN, porcentaje de aciertos en test con k =  8  : 0.5809576837416481\n",
      "k-NN, porcentaje de aciertos en entrenamiento con k =  10  : 0.5803697379166975\n",
      "k-NN, porcentaje de aciertos en test con k =  10  : 0.5740534521158129\n",
      "k-NN, porcentaje de aciertos en entrenamiento con k =  12  : 0.5721285915806668\n",
      "k-NN, porcentaje de aciertos en test con k =  12  : 0.56815144766147\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "k_values = [2,4,6,8,10,12];\n",
    "#Como ya hemos realizado en otras prácticas, vamos a ajustar el número de vecinos en k-NN, ya que se trata del parámetro más determinante\n",
    "#en relación al rendimiento del algoritmo. Además, la mayoría del resto de parámetros no eran numéricos.\n",
    "\n",
    "for i in range (0,6):\n",
    "    knn_classifier = neighbors.KNeighborsClassifier(n_neighbors = k_values[i])\n",
    "    knn_classifier.fit(tv_train_matrix, y_train)\n",
    "\n",
    "    knn_train_predictions = knn_classifier.predict(tv_train_matrix)\n",
    "    knn_test_predictions = knn_classifier.predict(tv_test_matrix)\n",
    "\n",
    "    print(\"k-NN, porcentaje de aciertos en entrenamiento con k = \",k_values[i], \" :\", np.mean(knn_train_predictions == y_train))\n",
    "    print(\"k-NN, porcentaje de aciertos en test con k = \",k_values[i], \" :\", np.mean(knn_test_predictions == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de k-NN, cuanto mayor es el numero de vecinos, peor es el rendimiento del algoritmo, quizás porque el problema que estamos\n",
    "analizando no tiene la complejidad suficiente para utilizar un número de vecinos mayor que 2. Con este número de vecinos es con el que se alcanza el rendimiento máximo, con un porcentaje del 72,31% en el entrenamiento y 64,79% en el test. Se trata de un rendimiento mejorable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\tf-env\\Lib\\site-packages\\sklearn\\naive_bayes.py:898: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes, porcentaje de aciertos en entrenamiento con alpha =  0  : 0.99784690771401\n",
      "Multinomial Naive Bayes, porcentaje de aciertos en test on alpha =  0  : 0.8407572383073497\n",
      "Multinomial Naive Bayes, porcentaje de aciertos en entrenamiento con alpha =  0.1  : 0.9725295122132304\n",
      "Multinomial Naive Bayes, porcentaje de aciertos en test on alpha =  0.1  : 0.9536748329621381\n",
      "Multinomial Naive Bayes, porcentaje de aciertos en entrenamiento con alpha =  0.3  : 0.9649565669314722\n",
      "Multinomial Naive Bayes, porcentaje de aciertos en test on alpha =  0.3  : 0.9501113585746103\n",
      "Multinomial Naive Bayes, porcentaje de aciertos en entrenamiento con alpha =  1  : 0.956381320068305\n",
      "Multinomial Naive Bayes, porcentaje de aciertos en test on alpha =  1  : 0.9454342984409799\n",
      "Multinomial Naive Bayes, porcentaje de aciertos en entrenamiento con alpha =  2  : 0.95315168163932\n",
      "Multinomial Naive Bayes, porcentaje de aciertos en test on alpha =  2  : 0.9436525612472161\n",
      "Multinomial Naive Bayes, porcentaje de aciertos en entrenamiento con alpha =  6  : 0.9481030514514812\n",
      "Multinomial Naive Bayes, porcentaje de aciertos en test on alpha =  6  : 0.9413140311804009\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "alpha_values = [0, 0.1, 0.3, 1, 2, 6]\n",
    "#Hemos elegido el parámetro alpha, que se encarga de determinar de que forma se suavizan las probabilidades condicionadas, ya que,\n",
    "#al fin y al cabo, Naive Bayes está basado en está suavización. El valor por defecto es 1, y hemos incluido valores menores y mayores que uno.\n",
    "for i in range (0,6):\n",
    "    mnb_classifier = MultinomialNB(alpha = alpha_values[i])\n",
    "\n",
    "    mnb_classifier.fit(tv_train_matrix, y_train)\n",
    "\n",
    "    mnb_train_predictions = mnb_classifier.predict(tv_train_matrix)\n",
    "    mnb_test_predictions = mnb_classifier.predict(tv_test_matrix)\n",
    "\n",
    "    print(\"Multinomial Naive Bayes, porcentaje de aciertos en entrenamiento con alpha = \",alpha_values[i], \" :\", np.mean(mnb_train_predictions == y_train))\n",
    "    print(\"Multinomial Naive Bayes, porcentaje de aciertos en test on alpha = \",alpha_values[i], \" :\", np.mean(mnb_test_predictions == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso del MultinomialNB, observamos un muy buen rendimiento del algoritmo independientemente del parámetro alpha, con un porcentaje de acierto siempre superior al 94%. Aún así, la tendencia es que a mayor valor de alpha, peor es el rendimiento del algoritmo, siendo el mejor porcentaje alcanzado 99,78%, con alpha = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GOs89bUHoiI"
   },
   "source": [
    "# 4) Construye redes neuronales con Keras con distintas maneras de usar word embeddings\n",
    "\n",
    "Justifica tus decisiones y explica los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar tres modelos, sin embeddings pre-entrenados, embeddings pre-entrenados sin congelar y embeddings pre-entrenados congelados. Por un lado, la opción sin embeddings preentrenados es útil porque permite al dominio aprender desde 0, directamente de los datos de entrenamiento. Por otro lado, los embeddings preentrenados no congelados parten de embeddings que ya tienen una base de entrenamiento, pero son capaces de adaptarse al problema en concreto, aprovechando lo que ya conocen (preentrenamiento) y adaptándolo al dominio específico. Finalmente, los embeddings preentrenados congelados ofrecen una base semántica adecuada procedente del preentrenamiento, y por ello, no se actualizan durante el entrenamiento. Su ventaja es que evitan el sobreajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 138021 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Parámetros\n",
    "max_words = 1500\n",
    "max_comment_length = 20\n",
    "\n",
    "# Tokenización\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df.text)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df.text)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# Padding\n",
    "data = pad_sequences(sequences, maxlen=max_comment_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 260,   43,   46, ...,   20,  567,  501],\n",
       "       [1365, 1042,    8, ...,  389,  567,  501],\n",
       "       [  17,    4,    3, ...,   20,  567,  501],\n",
       "       ...,\n",
       "       [  76,   40,  129, ...,  161,   13,   12],\n",
       "       [ 152,   85, 1138, ...,  510,  687,   16],\n",
       "       [   3,  179,   50, ...,    5,  331,  480]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Tras haber realizado la tokenizacion y el padding, dividimos los datos en entrenamiento, validacion y test al igual que hemos hecho\n",
    "#en el apartado 1.\n",
    "X_train, X_resto, y_train, y_resto = train_test_split(data, df.type, test_size=0.4, random_state=RANDOM_STATE, stratify = df.type)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_resto, y_resto, test_size=0.5, random_state=RANDOM_STATE, stratify = y_resto)\n",
    "\n",
    "# Fijamos el tamaño de los embedding a 50 dimensiones\n",
    "embedding_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\tf-env\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8647 - loss: 0.3592 - val_accuracy: 0.9595 - val_loss: 0.1149\n",
      "Epoch 2/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9678 - loss: 0.0905 - val_accuracy: 0.9639 - val_loss: 0.1043\n",
      "Epoch 3/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9822 - loss: 0.0577 - val_accuracy: 0.9639 - val_loss: 0.1039\n",
      "Epoch 4/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9921 - loss: 0.0351 - val_accuracy: 0.9671 - val_loss: 0.1064\n",
      "Epoch 5/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9969 - loss: 0.0193 - val_accuracy: 0.9660 - val_loss: 0.1138\n",
      "Epoch 6/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9994 - loss: 0.0092 - val_accuracy: 0.9664 - val_loss: 0.1224\n",
      "Epoch 7/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 0.9649 - val_loss: 0.1327\n",
      "Epoch 8/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.9657 - val_loss: 0.1407\n",
      "Epoch 9/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.9668 - val_loss: 0.1500\n",
      "Epoch 10/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 8.4392e-04 - val_accuracy: 0.9658 - val_loss: 0.1599\n",
      "Epoch 11/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 4.9791e-04 - val_accuracy: 0.9650 - val_loss: 0.1691\n",
      "Epoch 12/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 3.0505e-04 - val_accuracy: 0.9653 - val_loss: 0.1781\n",
      "Epoch 13/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 1.8804e-04 - val_accuracy: 0.9657 - val_loss: 0.1874\n",
      "Epoch 14/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 1.1925e-04 - val_accuracy: 0.9657 - val_loss: 0.1966\n",
      "Epoch 15/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 7.1659e-05 - val_accuracy: 0.9660 - val_loss: 0.2051\n",
      "Epoch 16/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 4.7083e-05 - val_accuracy: 0.9656 - val_loss: 0.2141\n",
      "Epoch 17/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 2.8561e-05 - val_accuracy: 0.9656 - val_loss: 0.2233\n",
      "Epoch 18/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 1.7604e-05 - val_accuracy: 0.9656 - val_loss: 0.2325\n",
      "Epoch 19/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 1.1288e-05 - val_accuracy: 0.9654 - val_loss: 0.2411\n",
      "Epoch 20/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 7.0828e-06 - val_accuracy: 0.9655 - val_loss: 0.2498\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9659 - loss: 0.2359\n",
      "Accuracy: 96.55%\n"
     ]
    }
   ],
   "source": [
    "# MODELO 1. SIN EMBEDDINGS PRE-ENTRENADOS\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model1 = Sequential()\n",
    "# We specify the maximum input length to our Embedding layer\n",
    "# so we can later flatten the embedded inputs\n",
    "\n",
    "\n",
    "model1.add(Embedding(max_words, embedding_dim, input_length=max_comment_length))\n",
    "# After the Embedding layer, our activations have shape `(max_words, max_comment_length, embedding_dim)`.\n",
    "\n",
    "# We flatten the 3D tensor of embeddings into a 2D tensor of shape `(max_words, max_comment_length * embedding_dim)`\n",
    "\n",
    "model1.add(Flatten())\n",
    "\n",
    "# We add the classifier on top\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model1.summary()\n",
    "\n",
    "history = model1.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "score1 = model1.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (score1[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos una accuracy del 96,55%, es decir, con 20 epochs conseguimos una red neuronal muy precisa, con un gran rendimiento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Inicializamos el Tfidf con df.text\n",
    "vectorizer = TfidfVectorizer(max_features = 1000) \n",
    "matriz = vectorizer.fit_transform(df.text)\n",
    "\n",
    "# Obtenemos las palabras\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Construimos el embeddings_index utilizando la matriz obtenida con el Tfidf\n",
    "embeddings_index = {}\n",
    "for idx, word in enumerate(words):\n",
    "    embeddings_index[word] = matriz.getcol(idx).toarray().flatten()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 44898\n",
    "\n",
    "#Construimos la matriz de embedding con el embedding index de la celda anterior\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < max_words:\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_words: 1500, embedding_dim: 44898, max_comment_length: 20\n",
      "embedding_matrix shape: (1500, 44898)\n",
      "Layer weights before setting: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\tf-env\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44898</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">67,347,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">897960</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">897,961</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m44898\u001b[0m)           │      \u001b[38;5;34m67,347,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m897960\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │         \u001b[38;5;34m897,961\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">68,244,961</span> (260.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m68,244,961\u001b[0m (260.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">68,244,961</span> (260.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m68,244,961\u001b[0m (260.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m646s\u001b[0m 766ms/step - accuracy: 0.8935 - loss: 0.4511 - val_accuracy: 0.9335 - val_loss: 0.6334\n",
      "Epoch 2/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m639s\u001b[0m 759ms/step - accuracy: 0.9628 - loss: 0.3275 - val_accuracy: 0.9425 - val_loss: 1.0698\n",
      "Epoch 3/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m648s\u001b[0m 769ms/step - accuracy: 0.9772 - loss: 0.2867 - val_accuracy: 0.9478 - val_loss: 1.3289\n",
      "Epoch 4/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m647s\u001b[0m 769ms/step - accuracy: 0.9835 - loss: 0.2614 - val_accuracy: 0.9542 - val_loss: 1.6407\n",
      "Epoch 5/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m640s\u001b[0m 761ms/step - accuracy: 0.9862 - loss: 0.2408 - val_accuracy: 0.9513 - val_loss: 2.1891\n",
      "Epoch 6/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m623s\u001b[0m 739ms/step - accuracy: 0.9897 - loss: 0.2285 - val_accuracy: 0.9539 - val_loss: 2.8219\n",
      "Epoch 7/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5059s\u001b[0m 6s/step - accuracy: 0.9924 - loss: 0.1571 - val_accuracy: 0.9545 - val_loss: 3.1260\n",
      "Epoch 8/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m629s\u001b[0m 747ms/step - accuracy: 0.9916 - loss: 0.2270 - val_accuracy: 0.9551 - val_loss: 3.6509\n",
      "Epoch 9/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m627s\u001b[0m 744ms/step - accuracy: 0.9950 - loss: 0.1396 - val_accuracy: 0.9545 - val_loss: 4.0101\n",
      "Epoch 10/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1115s\u001b[0m 1s/step - accuracy: 0.9950 - loss: 0.1487 - val_accuracy: 0.9549 - val_loss: 4.7460\n",
      "Epoch 11/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m693s\u001b[0m 823ms/step - accuracy: 0.9942 - loss: 0.2288 - val_accuracy: 0.9614 - val_loss: 4.8589\n",
      "Epoch 12/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m731s\u001b[0m 868ms/step - accuracy: 0.9946 - loss: 0.1557 - val_accuracy: 0.9575 - val_loss: 5.5757\n",
      "Epoch 13/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 765ms/step - accuracy: 0.9948 - loss: 0.1615 - val_accuracy: 0.9590 - val_loss: 5.8795\n",
      "Epoch 14/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m638s\u001b[0m 758ms/step - accuracy: 0.9956 - loss: 0.1813 - val_accuracy: 0.9571 - val_loss: 6.5482\n",
      "Epoch 15/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m635s\u001b[0m 754ms/step - accuracy: 0.9959 - loss: 0.1731 - val_accuracy: 0.9561 - val_loss: 7.2498\n",
      "Epoch 16/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m648s\u001b[0m 770ms/step - accuracy: 0.9982 - loss: 0.0556 - val_accuracy: 0.9605 - val_loss: 7.3861\n",
      "Epoch 17/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m635s\u001b[0m 755ms/step - accuracy: 0.9974 - loss: 0.1054 - val_accuracy: 0.9546 - val_loss: 8.1994\n",
      "Epoch 18/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m636s\u001b[0m 756ms/step - accuracy: 0.9969 - loss: 0.1355 - val_accuracy: 0.9588 - val_loss: 8.4126\n",
      "Epoch 19/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m638s\u001b[0m 758ms/step - accuracy: 0.9965 - loss: 0.1495 - val_accuracy: 0.9541 - val_loss: 9.3192\n",
      "Epoch 20/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m637s\u001b[0m 757ms/step - accuracy: 0.9981 - loss: 0.1104 - val_accuracy: 0.9537 - val_loss: 9.7275\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - accuracy: 0.9503 - loss: 9.7418\n"
     ]
    }
   ],
   "source": [
    "# MODELO 2. EMBEDDINGS PRE-ENTRENADOS CONGELADOS\n",
    "\n",
    "# from keras.models import Sequential  # DEPRECATED\n",
    "from tensorflow.keras.models import Sequential\n",
    "# from keras.layers import Embedding, Flatten, Dense  # DEPRECATED\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "import numpy as np #####\n",
    "\n",
    "# Debugging prints\n",
    "print(f\"max_words: {max_words}, embedding_dim: {embedding_dim}, max_comment_length: {max_comment_length}\")\n",
    "print(f\"embedding_matrix shape: {embedding_matrix.shape}\")\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(max_words, embedding_dim, input_length=max_comment_length))\n",
    "# model2.add(Embedding(max_words, embedding_dim, input_length=max_comment_length, weights=embedding_matrix))\n",
    "\n",
    "# Check if weights exist before setting them\n",
    "print(\"Layer weights before setting:\", model2.layers[0].weights)\n",
    "\n",
    "# Build the model (just in case)\n",
    "model2.build(input_shape=(None, max_comment_length))\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model2.summary()\n",
    "\n",
    "history = model2.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "score2 = model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conseguimos una accuracy del 95,03%,  volvemos a tener una red neuronal muy buena, entrenándola con 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m696s\u001b[0m 824ms/step - accuracy: 0.8565 - loss: 1.5660 - val_accuracy: 0.9131 - val_loss: 0.8803\n",
      "Epoch 2/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m664s\u001b[0m 789ms/step - accuracy: 0.9607 - loss: 0.3489 - val_accuracy: 0.9433 - val_loss: 0.6766\n",
      "Epoch 3/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m640s\u001b[0m 761ms/step - accuracy: 0.9793 - loss: 0.1817 - val_accuracy: 0.9486 - val_loss: 0.9185\n",
      "Epoch 4/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m654s\u001b[0m 777ms/step - accuracy: 0.9842 - loss: 0.1633 - val_accuracy: 0.9528 - val_loss: 1.2371\n",
      "Epoch 5/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m660s\u001b[0m 784ms/step - accuracy: 0.9847 - loss: 0.1835 - val_accuracy: 0.9500 - val_loss: 1.7998\n",
      "Epoch 6/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m659s\u001b[0m 783ms/step - accuracy: 0.9877 - loss: 0.2054 - val_accuracy: 0.9487 - val_loss: 2.4158\n",
      "Epoch 7/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m672s\u001b[0m 799ms/step - accuracy: 0.9881 - loss: 0.2152 - val_accuracy: 0.9542 - val_loss: 3.0993\n",
      "Epoch 8/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m654s\u001b[0m 777ms/step - accuracy: 0.9922 - loss: 0.1825 - val_accuracy: 0.9565 - val_loss: 3.6131\n",
      "Epoch 9/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m659s\u001b[0m 782ms/step - accuracy: 0.9942 - loss: 0.1582 - val_accuracy: 0.9584 - val_loss: 3.9874\n",
      "Epoch 10/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m656s\u001b[0m 780ms/step - accuracy: 0.9961 - loss: 0.1037 - val_accuracy: 0.9587 - val_loss: 4.5756\n",
      "Epoch 11/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m632s\u001b[0m 751ms/step - accuracy: 0.9960 - loss: 0.1379 - val_accuracy: 0.9577 - val_loss: 5.0511\n",
      "Epoch 12/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m632s\u001b[0m 751ms/step - accuracy: 0.9958 - loss: 0.1478 - val_accuracy: 0.9594 - val_loss: 5.7086\n",
      "Epoch 13/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m631s\u001b[0m 750ms/step - accuracy: 0.9943 - loss: 0.2201 - val_accuracy: 0.9550 - val_loss: 6.0253\n",
      "Epoch 14/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m631s\u001b[0m 749ms/step - accuracy: 0.9963 - loss: 0.1226 - val_accuracy: 0.9572 - val_loss: 6.3685\n",
      "Epoch 15/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m632s\u001b[0m 751ms/step - accuracy: 0.9968 - loss: 0.1468 - val_accuracy: 0.9600 - val_loss: 7.1768\n",
      "Epoch 16/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m632s\u001b[0m 750ms/step - accuracy: 0.9967 - loss: 0.1800 - val_accuracy: 0.9590 - val_loss: 7.3495\n",
      "Epoch 17/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m631s\u001b[0m 750ms/step - accuracy: 0.9963 - loss: 0.1712 - val_accuracy: 0.9577 - val_loss: 8.2873\n",
      "Epoch 18/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m634s\u001b[0m 753ms/step - accuracy: 0.9968 - loss: 0.1633 - val_accuracy: 0.9561 - val_loss: 8.6807\n",
      "Epoch 19/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m633s\u001b[0m 752ms/step - accuracy: 0.9978 - loss: 0.1650 - val_accuracy: 0.9570 - val_loss: 8.8620\n",
      "Epoch 20/20\n",
      "\u001b[1m842/842\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m632s\u001b[0m 751ms/step - accuracy: 0.9984 - loss: 0.0621 - val_accuracy: 0.9581 - val_loss: 9.2178\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 30ms/step - accuracy: 0.9614 - loss: 8.3629\n"
     ]
    }
   ],
   "source": [
    "# MODELO3. EMBEDDINGS PREENTRENADOS SIN CONGELAR\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(max_words, embedding_dim, input_length=max_comment_length))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model3.summary()\n",
    "\n",
    "model3.build(input_shape=(None, max_comment_length)) #####\n",
    "model3.layers[0].set_weights([embedding_matrix])\n",
    "model3.layers[0].trainable = True\n",
    "\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model3.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "score3 = model3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conseguimos una accuracy del 96%, teniendo de nuevo una red neuronal muy buena, entrenándola con 20 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFoiJYlCHoiJ"
   },
   "source": [
    "# 5) Aplica los modelos construidos a los datos de test y compáralos.\n",
    "\n",
    "Calcula las métricas de recall, precisión y f1.\n",
    "Discute cual es el mejor modelo y cual es peor y porqué."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "=== Métricas del Modelo 1 ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9642    0.9689    0.9665      4696\n",
      "           1     0.9657    0.9606    0.9631      4284\n",
      "\n",
      "    accuracy                         0.9649      8980\n",
      "   macro avg     0.9650    0.9647    0.9648      8980\n",
      "weighted avg     0.9649    0.9649    0.9649      8980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred1 = (model1.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "print(\"=== Métricas del Modelo 1 ===\")\n",
    "print(classification_report(y_test, y_pred1, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 27ms/step\n",
      "=== Métricas del Modelo 2 ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9436    0.9693    0.9563      4696\n",
      "           1     0.9654    0.9365    0.9507      4284\n",
      "\n",
      "    accuracy                         0.9537      8980\n",
      "   macro avg     0.9545    0.9529    0.9535      8980\n",
      "weighted avg     0.9540    0.9537    0.9536      8980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = (model2.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "print(\"=== Métricas del Modelo 2 ===\")\n",
    "print(classification_report(y_test, y_pred2, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 27ms/step\n",
      "=== Métricas del Modelo 3 ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9563    0.9640    0.9601      4696\n",
      "           1     0.9602    0.9517    0.9559      4284\n",
      "\n",
      "    accuracy                         0.9581      8980\n",
      "   macro avg     0.9582    0.9578    0.9580      8980\n",
      "weighted avg     0.9581    0.9581    0.9581      8980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred3 = (model3.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "print(\"=== Métricas del Modelo 3 ===\")\n",
    "print(classification_report(y_test, y_pred3, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, todos los modelos tienen un gran rendimiento ya que, como podemos observar las tres métricas superan el 90% en los tres modelos de redes neuronales que hemos construido.\n",
    "Precisando más, el primer modelo, sin embeddings pre-entrenados es el que, en general, mejor rendimiento tiene, con la media de las tres métricas superando el 96%, mientras que el segundo (embeddings pre-entrenados congelados) y tercer modelo (embeddings pre-entrenados sin congelar) le siguen muy de cerca, con medias del 95% en las métricas, respectivamente.\n",
    "En cuanto a si las noticias son verdaderas o falsas, no se observa ninguna diferencia notable en las métricas, ya que, ninguna de ellas se predice mejor que la otra."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11 (TensorFlow)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
